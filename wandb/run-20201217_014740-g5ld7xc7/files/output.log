2020-12-17 01:47:41.744 INFO:	gpus = [0]
2020-12-17 01:47:41.830 INFO:	gpu device = 0
2020-12-17 01:47:41.830 INFO:	args = Namespace(arch_learning_rate=0.0003, arch_search_method='DARTS', arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=50, gpu='0', grad_clip=5, group_id=2, init_channels=16, lambda_train_regularizer=1, lambda_valid_regularizer=1.0, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, optimization='SINGLE', report_freq=50, run_id=77723, save='search-EXP-20201217-014740', seed=2, train_portion=0.5, unrolled=True, weight_decay=0.0003)
2020-12-17 01:47:43.647 INFO:	param size = 1.930842MB
Files already downloaded and verified
/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2020-12-17 01:47:44.306 INFO:	epoch 0 lr 2.500000e-02
2020-12-17 01:47:46.278 INFO:	train 000 2.475629e+00 9.375000 43.750000
Traceback (most recent call last):
  File "./search_algorithm/train_single_level.py", line 304, in <module>
    main()
  File "./search_algorithm/train_single_level.py", line 178, in main
    train_acc, train_obj, train_loss = train(epoch, train_queue, valid_queue, model, architect, criterion, optimizer, lr)
  File "./search_algorithm/train_single_level.py", line 258, in train
    loss.backward()
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/tensor.py", line 221, in backward
    torch.autograd.backward(self, gradient, retain_graph, create_graph)
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/autograd/__init__.py", line 130, in backward
    Variable._execution_engine.run_backward(
KeyboardInterrupt
