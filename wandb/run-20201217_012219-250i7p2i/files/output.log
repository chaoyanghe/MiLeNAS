2020-12-17 01:22:20.922 INFO:	gpus = [1]
2020-12-17 01:22:20.992 INFO:	gpu device = 1
2020-12-17 01:22:20.992 INFO:	args = Namespace(arch_learning_rate=0.0003, arch_search_method='DARTS', arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=50, gpu='1', grad_clip=5, group_id=1, init_channels=16, lambda_train_regularizer=1.0, lambda_valid_regularizer=1.0, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, optimization='DARTS', report_freq=50, run_id=77715, save='search-EXP-20201217-012219', seed=2, train_portion=0.5, unrolled=1, weight_decay=0.0003)
2020-12-17 01:22:22.757 INFO:	param size = 1.930842MB
Files already downloaded and verified
/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2020-12-17 01:22:23.419 INFO:	epoch 0 lr 2.500000e-02
/home/chaoyanghe/sourcecode/MiLeNAS/search_algorithm/architect.py:48: UserWarning: This overload of sub is deprecated:
	sub(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	sub(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  unrolled_model = self._construct_model_from_theta(theta.sub(eta, moment + dtheta))
2020-12-17 01:22:29.299 INFO:	train 000 2.475933e+00 9.375000 43.750000
Traceback (most recent call last):
  File "./search_algorithm/train_darts.py", line 321, in <module>
  File "./search_algorithm/train_darts.py", line 172, in main
    train_acc, train_obj, train_loss = train(epoch, train_queue, valid_queue, model, architect, criterion, optimizer, lr)
  File "./search_algorithm/train_darts.py", line 276, in train
    prec1, prec5 = utils.accuracy(logits, target, topk=(1, 5))
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py", line 67, in wrapper
    return wrapped(*args, **kwargs)
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/autograd/grad_mode.py", line 26, in decorate_context
    return func(*args, **kwargs)
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/sgd.py", line 97, in step
    d_p = p.grad
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/tensor.py", line 947, in grad
    if self.requires_grad and not hasattr(self, "retains_grad") and not self.is_leaf and self._grad is None:
KeyboardInterrupt
