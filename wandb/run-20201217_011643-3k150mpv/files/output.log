2020-12-17 01:16:44.549 INFO:	gpus = [1]
2020-12-17 01:16:44.617 INFO:	gpu device = 1
2020-12-17 01:16:44.618 INFO:	args = Namespace(arch_learning_rate=0.0003, arch_search_method='DARTS', arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, epochs=50, gpu='1', grad_clip=5, group_id=1, init_channels=16, lambda_train_regularizer=1.0, lambda_valid_regularizer=1.0, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, optimization='DARTS', report_freq=50, run_id=77715, save='search-EXP-20201217-011642', seed=2, train_portion=0.5, unrolled=1, weight_decay=0.0003)
2020-12-17 01:16:46.284 INFO:	param size = 1.930842MB
Files already downloaded and verified
/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2020-12-17 01:16:46.929 INFO:	epoch 0 lr 2.495266e-02
/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
/home/chaoyanghe/sourcecode/MiLeNAS/search_algorithm/architect.py:48: UserWarning: This overload of sub is deprecated:
	sub(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	sub(Tensor other, *, Number alpha) (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/torch/csrc/utils/python_arg_parser.cpp:882.)
  unrolled_model = self._construct_model_from_theta(theta.sub(eta, moment + dtheta))
2020-12-17 01:16:52.702 INFO:	train 000 2.475924e+00 9.375000 43.750000
Traceback (most recent call last):
Traceback (most recent call last):
  File "./search_algorithm/train_darts.py", line 320, in <module>
    if __name__ == '__main__':
  File "./search_algorithm/train_darts.py", line 173, in main
    logging.info('train_acc %f', train_acc)
  File "./search_algorithm/train_darts.py", line 262, in train
    order_optinion = False
  File "/home/chaoyanghe/sourcecode/MiLeNAS/search_algorithm/architect.py", line 56, in step
    self._backward_step_unrolled(input_train, target_train, input_valid, target_valid, eta, network_optimizer)
  File "/home/chaoyanghe/sourcecode/MiLeNAS/search_algorithm/architect.py", line 190, in _backward_step_unrolled
    implicit_grads = self._hessian_vector_product(vector, input_train, target_train)
  File "/home/chaoyanghe/sourcecode/MiLeNAS/search_algorithm/architect.py", line 257, in _hessian_vector_product
    grads_n = torch.autograd.grad(loss, arch_parameters)
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/autograd/__init__.py", line 202, in grad
    return Variable._execution_engine.run_backward(
KeyboardInterrupt
