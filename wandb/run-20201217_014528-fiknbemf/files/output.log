2020-12-17 01:45:29.542 INFO:	gpus = [0]
2020-12-17 01:45:29.634 INFO:	gpu device = 0
2020-12-17 01:45:29.634 INFO:	args = Namespace(arch_learning_rate=0.0003, arch_search_method='DARTS', arch_weight_decay=0.001, batch_size=64, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.3, early_stopping=0, epochs=50, gpu='0', grad_clip=5, group_id=11, init_channels=16, lambda_train_regularizer=1.0, lambda_valid_regularizer=0.2, layers=8, learning_rate=0.05, learning_rate_min=0.001, mixed_portion=0.45, model_path='saved_models', momentum=0.9, optimization='V2', report_freq=50, run_id=44445, save='search-EXP-20201217-014528', seed=2, train_portion=0.5, unrolled=True, w_update_times=1, weight_decay=0.0003)
2020-12-17 01:45:31.478 INFO:	param size = 1.930842MB
Files already downloaded and verified
/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
2020-12-17 01:45:32.154 INFO:	epoch 0 lr 4.990336e-02
/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:508: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.
  warnings.warn("To get the last learning rate computed by the scheduler, "
2020-12-17 01:45:34.740 INFO:	train 000 2.520615e+00 7.812500 43.750000
Traceback (most recent call last):
  File "./search_algorithm/train_milenas_multiple_w_update.py", line 343, in <module>
    if __name__ == '__main__':
  File "./search_algorithm/train_milenas_multiple_w_update.py", line 191, in main
    if is_wandb_used:
  File "./search_algorithm/train_milenas_multiple_w_update.py", line 297, in train
    parameters = model.module.arch_parameters() if is_multi_gpu else model.arch_parameters()
  File "/home/chaoyanghe/miniconda/envs/milenas/lib/python3.8/site-packages/torch/nn/utils/clip_grad.py", line 38, in clip_grad_norm_
    if clip_coef < 1:
KeyboardInterrupt
